{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitad52f44d3ef14f22b4dcb0bcf62623bd",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert\n",
    "Lets ***transform the game***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get some imports up in here\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "import math as m    \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a few lil' hyperparameters\n",
    "sup_params = {\"LAYERS\": 24,\n",
    "              \"D_HIDDEN\": 1024,\n",
    "              \"HEADS\": 16,\n",
    "              \"EPOCHS\": 10, \n",
    "              \"LR\":3e-5, # Adam optimiser\n",
    "              \"BATCH_SIZE\": 64,\n",
    "              \"DROP_PROB\": 0.1}\n",
    "few_params = {\"LAYERS\": 24,\n",
    "              \"D_HIDDEN\": 1024,\n",
    "              \"HEADS\": 16,\n",
    "              \"EPOCHS\": 10, \n",
    "              \"LR\":1e-4, # SGD optimiser\n",
    "              \"BATCH_SIZE\": 256,\n",
    "              \"DROP_PROB\": 0.1}\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a multihead attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Toy Encodings:\n tensor([[[0.0000, 0.1000, 0.2000, 0.3000],\n         [1.0000, 1.1000, 1.2000, 1.3000],\n         [2.0000, 2.1000, 2.2000, 2.3000]]])\nToy MHA: \n tensor([[[-0.0925, -0.0732, -0.0211,  0.1079],\n         [-0.5046, -0.4852, -0.0248,  0.7104],\n         [-0.9168, -0.8971, -0.0285,  1.3128]]], grad_fn=<AddBackward0>)\nToy MHA Shape: \n torch.Size([1, 3, 4])\n"
    }
   ],
   "source": [
    "toy_encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) \n",
    "# shape(toy_encodings) = [B, T, D] = (1, 3, 4)\n",
    "print(\"Toy Encodings:\\n\", toy_encodings)\n",
    "\n",
    "D_MODEL = toy_encodings.shape[-1]\n",
    "\n",
    "toy_MHA_layer = nn.MultiheadAttention(embed_dim=D_MODEL, num_heads=2)\n",
    "toy_MHA, _ = toy_MHA_layer(toy_encodings, toy_encodings, toy_encodings)\n",
    "print(\"Toy MHA: \\n\", toy_MHA)\n",
    "print(\"Toy MHA Shape: \\n\", toy_MHA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a layer normalisation module that includes dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_hidden=sup_params[\"D_HIDDEN\"], dropout=sup_params[\"DROP_PROB\"]):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ln = self.layer_norm(x)\n",
    "        return self.dropout(ln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position-wise feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWFFN(nn.Module):\n",
    "    def __init__(self, d_hidden=sup_params[\"D_HIDDEN\"], d_ff=4*sup_params[\"D_HIDDEN\"], dropout=sup_params[\"DROP_PROB\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_hidden, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_hidden)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape(x) = [B x seq_len x D]\n",
    "\n",
    "        return self.ff(x)\n",
    "        # shape(ff) = [B x seq_len x D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Transformer(d_model=sup_params[\"D_HIDDEN\"], nhead=sup_params[\"HEADS\"])"
   ]
  }
 ]
}